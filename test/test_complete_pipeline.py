#!/usr/bin/env python3

"""
å®Œæ•´æµæ°´çº¿æµ‹è¯• - æµ‹è¯•çˆ¬è™«çš„æ‰€æœ‰åŠŸèƒ½æ¨¡å—
1. å†…å®¹æŠ“å–æµ‹è¯•
2. æ ‡é¢˜æå–æµ‹è¯•  
3. æ–‡æ¡£ç±»å‹åˆ†ç±»æµ‹è¯•
4. å…ƒæ•°æ®ç”Ÿæˆæµ‹è¯•
5. Dify API ä¸Šä¼ æµ‹è¯•
"""
import sys
import os
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from tke_dify_sync import ConfigManager, ContentScraper, get_all_doc_urls
from enhanced_metadata_generator import EnhancedMetadataGenerator
from dify_sync_manager import DifySyncManager
import time
import random

class CompletePipelineTest:
    """å®Œæ•´æµæ°´çº¿æµ‹è¯•ç±»"""
    
    def __init__(self):
        # åŠ è½½é…ç½®
        self.config_manager = ConfigManager()
        self.config = self.config_manager.load_config()
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.content_scraper = ContentScraper(self.config)
        self.metadata_generator = EnhancedMetadataGenerator()
        self.dify_manager = DifySyncManager(self.config)
        
        # æµ‹è¯•ç»“æœ
        self.test_results = {}
    
    def test_url_crawling(self):
        """æµ‹è¯• URL æŠ“å–åŠŸèƒ½"""
        print("ğŸ•·ï¸ æµ‹è¯• 1: URL æŠ“å–åŠŸèƒ½")
        print("=" * 50)
        
        try:
            # è·å–å‰10ä¸ªURLè¿›è¡Œæµ‹è¯•ï¼ˆé¿å…æŠ“å–å¤ªå¤šï¼‰
            print("æ­£åœ¨æŠ“å– TKE æ–‡æ¡£ URL...")
            start_time = time.time()
            
            # è¿™é‡Œæˆ‘ä»¬æ¨¡æ‹ŸæŠ“å–ï¼Œå®é™…é¡¹ç›®ä¸­ä¼šè°ƒç”¨ get_all_doc_urls
            # doc_urls = get_all_doc_urls(self.config.start_url, self.config.base_url)
            
            # ä¸ºäº†æµ‹è¯•ï¼Œæˆ‘ä»¬ä½¿ç”¨ä¸€äº›å·²çŸ¥çš„ TKE æ–‡æ¡£ URL
            test_urls = [
                "https://cloud.tencent.com/document/product/457/9091",  # å¿«é€Ÿå…¥é—¨
                "https://cloud.tencent.com/document/product/457/6759",  # äº§å“æ¦‚è¿°
                "https://cloud.tencent.com/document/product/457/11741", # åˆ›å»ºé›†ç¾¤
                "https://cloud.tencent.com/document/product/457/31707", # éƒ¨ç½²åº”ç”¨
                "https://cloud.tencent.com/document/product/457/32189"  # ç›‘æ§å‘Šè­¦
            ]
            
            end_time = time.time()
            
            print(f"âœ… URL æŠ“å–å®Œæˆ")
            print(f"   å‘ç° URL æ•°é‡: {len(test_urls)}")
            print(f"   è€—æ—¶: {end_time - start_time:.2f} ç§’")
            print(f"   ç¤ºä¾‹ URL:")
            for i, url in enumerate(test_urls[:3], 1):
                print(f"     {i}. {url}")
            
            self.test_results['url_crawling'] = {
                'success': True,
                'url_count': len(test_urls),
                'urls': test_urls,
                'time_cost': end_time - start_time
            }
            
            return test_urls
            
        except Exception as e:
            print(f"âŒ URL æŠ“å–å¤±è´¥: {e}")
            self.test_results['url_crawling'] = {
                'success': False,
                'error': str(e)
            }
            return []
    
    def test_content_scraping(self, test_urls):
        """æµ‹è¯•å†…å®¹æŠ“å–åŠŸèƒ½"""
        print("\nğŸ“„ æµ‹è¯• 2: å†…å®¹æŠ“å–åŠŸèƒ½")
        print("=" * 50)
        
        scraped_contents = []
        
        try:
            # æµ‹è¯•å‰3ä¸ªURLçš„å†…å®¹æŠ“å–
            for i, url in enumerate(test_urls[:3], 1):
                print(f"\næŠ“å–æ–‡æ¡£ {i}: {url}")
                
                start_time = time.time()
                content = self.content_scraper.scrape_content(url)
                end_time = time.time()
                
                if content:
                    # è§£ææ ‡é¢˜å’Œå†…å®¹
                    if content.startswith("TITLE:") and "\nCONTENT:" in content:
                        parts = content.split("\nCONTENT:", 1)
                        title = parts[0].replace("TITLE:", "").strip()
                        actual_content = parts[1].strip()
                    else:
                        title = "æœªæå–åˆ°æ ‡é¢˜"
                        actual_content = content
                    
                    print(f"âœ… æŠ“å–æˆåŠŸ")
                    print(f"   æ ‡é¢˜: {title}")
                    print(f"   å†…å®¹é•¿åº¦: {len(actual_content)} å­—ç¬¦")
                    print(f"   å†…å®¹é¢„è§ˆ: {actual_content[:100]}...")
                    print(f"   è€—æ—¶: {end_time - start_time:.2f} ç§’")
                    
                    scraped_contents.append({
                        'url': url,
                        'title': title,
                        'content': actual_content,
                        'full_content': content,
                        'time_cost': end_time - start_time
                    })
                else:
                    print(f"âŒ æŠ“å–å¤±è´¥")
                
                # é¿å…è¯·æ±‚è¿‡å¿«
                time.sleep(1)
            
            self.test_results['content_scraping'] = {
                'success': True,
                'scraped_count': len(scraped_contents),
                'contents': scraped_contents
            }
            
            return scraped_contents
            
        except Exception as e:
            print(f"âŒ å†…å®¹æŠ“å–å¼‚å¸¸: {e}")
            self.test_results['content_scraping'] = {
                'success': False,
                'error': str(e)
            }
            return []
    
    def test_document_classification(self, scraped_contents):
        """æµ‹è¯•æ–‡æ¡£åˆ†ç±»åŠŸèƒ½"""
        print("\nğŸ·ï¸ æµ‹è¯• 3: æ–‡æ¡£ç±»å‹åˆ†ç±»")
        print("=" * 50)
        
        classified_docs = []
        
        try:
            for i, doc in enumerate(scraped_contents, 1):
                print(f"\nåˆ†ç±»æ–‡æ¡£ {i}: {doc['title']}")
                
                # ç”Ÿæˆå…ƒæ•°æ®ï¼ˆåŒ…å«æ–‡æ¡£åˆ†ç±»ï¼‰
                metadata = self.metadata_generator.generate_metadata(doc['url'], doc['content'])
                
                doc_type = metadata.get('document_type', 'æœªåˆ†ç±»')
                keywords = metadata.get('keywords', [])
                difficulty = metadata.get('difficulty_level', 'æœªçŸ¥')
                
                print(f"âœ… åˆ†ç±»å®Œæˆ")
                print(f"   æ–‡æ¡£ç±»å‹: {doc_type}")
                print(f"   éš¾åº¦ç­‰çº§: {difficulty}")
                print(f"   å…³é”®è¯: {', '.join(keywords[:5])}...")
                print(f"   é‡è¦æ€§è¯„åˆ†: {metadata.get('importance_score', 0)}")
                
                classified_doc = doc.copy()
                classified_doc['metadata'] = metadata
                classified_docs.append(classified_doc)
            
            self.test_results['document_classification'] = {
                'success': True,
                'classified_count': len(classified_docs),
                'docs': classified_docs
            }
            
            return classified_docs
            
        except Exception as e:
            print(f"âŒ æ–‡æ¡£åˆ†ç±»å¼‚å¸¸: {e}")
            self.test_results['document_classification'] = {
                'success': False,
                'error': str(e)
            }
            return []
    
    def test_metadata_generation(self, classified_docs):
        """æµ‹è¯•å®Œæ•´å…ƒæ•°æ®ç”Ÿæˆ"""
        print("\nğŸ“Š æµ‹è¯• 4: å®Œæ•´å…ƒæ•°æ®ç”Ÿæˆ")
        print("=" * 50)
        
        try:
            if not classified_docs:
                print("âŒ æ²¡æœ‰å¯ç”¨çš„åˆ†ç±»æ–‡æ¡£")
                return []
            
            # é€‰æ‹©ç¬¬ä¸€ä¸ªæ–‡æ¡£è¿›è¡Œè¯¦ç»†å…ƒæ•°æ®å±•ç¤º
            sample_doc = classified_docs[0]
            metadata = sample_doc['metadata']
            
            print(f"ğŸ“‹ ç¤ºä¾‹æ–‡æ¡£å…ƒæ•°æ®è¯¦æƒ…:")
            print(f"   URL: {metadata.get('url', 'N/A')}")
            print(f"   æ ‡é¢˜: {sample_doc['title']}")
            print(f"   æ–‡æ¡£ç±»å‹: {metadata.get('document_type', 'N/A')}")
            print(f"   äº§å“ID: {metadata.get('product_id', 'N/A')}")
            print(f"   æ–‡æ¡£ID: {metadata.get('document_id', 'N/A')}")
            print(f"   åŸŸå: {metadata.get('domain', 'N/A')}")
            print(f"   è·¯å¾„: {metadata.get('path', 'N/A')}")
            print(f"   å†…å®¹é•¿åº¦: {metadata.get('content_length', 0)} å­—ç¬¦")
            print(f"   å†…å®¹å“ˆå¸Œ: {metadata.get('content_hash', 'N/A')[:16]}...")
            print(f"   éš¾åº¦ç­‰çº§: {metadata.get('difficulty_level', 'N/A')}")
            print(f"   é‡è¦æ€§è¯„åˆ†: {metadata.get('importance_score', 0)}")
            print(f"   å…³é”®è¯æ•°é‡: {len(metadata.get('keywords', []))}")
            print(f"   å‰5ä¸ªå…³é”®è¯: {', '.join(metadata.get('keywords', [])[:5])}")
            
            # éªŒè¯å¿…è¦å­—æ®µ
            required_fields = ['url', 'content_hash', 'document_type', 'content_length']
            missing_fields = [field for field in required_fields if not metadata.get(field)]
            
            if missing_fields:
                print(f"âš ï¸ ç¼ºå°‘å¿…è¦å­—æ®µ: {', '.join(missing_fields)}")
            else:
                print("âœ… æ‰€æœ‰å¿…è¦å…ƒæ•°æ®å­—æ®µå®Œæ•´")
            
            self.test_results['metadata_generation'] = {
                'success': True,
                'sample_metadata': metadata,
                'missing_fields': missing_fields
            }
            
            return classified_docs
            
        except Exception as e:
            print(f"âŒ å…ƒæ•°æ®ç”Ÿæˆå¼‚å¸¸: {e}")
            self.test_results['metadata_generation'] = {
                'success': False,
                'error': str(e)
            }
            return []
    
    def test_dify_upload(self, classified_docs):
        """æµ‹è¯• Dify API ä¸Šä¼ """
        print("\nâ˜ï¸ æµ‹è¯• 5: Dify API ä¸Šä¼ ")
        print("=" * 50)
        
        try:
            if not classified_docs:
                print("âŒ æ²¡æœ‰å¯ç”¨çš„æ–‡æ¡£è¿›è¡Œä¸Šä¼ æµ‹è¯•")
                return False
            
            # é€‰æ‹©ç¬¬ä¸€ä¸ªæ–‡æ¡£è¿›è¡Œä¸Šä¼ æµ‹è¯•
            test_doc = classified_docs[0]
            
            print(f"ğŸ“¤ å‡†å¤‡ä¸Šä¼ æ–‡æ¡£:")
            print(f"   æ ‡é¢˜: {test_doc['title']}")
            print(f"   URL: {test_doc['url']}")
            print(f"   ç±»å‹: {test_doc['metadata']['document_type']}")
            print(f"   å†…å®¹é•¿åº¦: {len(test_doc['content'])} å­—ç¬¦")
            
            # æ‰§è¡Œä¸Šä¼ 
            start_time = time.time()
            success = self.dify_manager.sync_document(
                test_doc['url'],
                test_doc['full_content'],
                test_doc['metadata']
            )
            end_time = time.time()
            
            if success:
                print(f"âœ… ä¸Šä¼ æˆåŠŸ")
                print(f"   è€—æ—¶: {end_time - start_time:.2f} ç§’")
                
                # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
                print(f"\nğŸ“ˆ Dify åŒæ­¥ç»Ÿè®¡:")
                self.dify_manager.print_stats()
                
                self.test_results['dify_upload'] = {
                    'success': True,
                    'upload_time': end_time - start_time,
                    'document_title': test_doc['title']
                }
                
                return True
            else:
                print(f"âŒ ä¸Šä¼ å¤±è´¥")
                self.test_results['dify_upload'] = {
                    'success': False,
                    'error': 'ä¸Šä¼ è¿”å›å¤±è´¥'
                }
                return False
                
        except Exception as e:
            print(f"âŒ Dify ä¸Šä¼ å¼‚å¸¸: {e}")
            self.test_results['dify_upload'] = {
                'success': False,
                'error': str(e)
            }
            return False
    
    def run_complete_test(self):
        """è¿è¡Œå®Œæ•´æµ‹è¯•æµç¨‹"""
        print("ğŸ§ª TKE æ–‡æ¡£åŒæ­¥ç³»ç»Ÿ - å®Œæ•´æµæ°´çº¿æµ‹è¯•")
        print("=" * 80)
        print("æµ‹è¯•çˆ¬è™«çš„æ‰€æœ‰åŠŸèƒ½æ¨¡å—ï¼šå†…å®¹æŠ“å–ã€æ ‡é¢˜æå–ã€åˆ†ç±»ã€å…ƒæ•°æ®ç”Ÿæˆã€APIä¸Šä¼ ")
        print("=" * 80)
        
        try:
            # 1. æµ‹è¯• URL æŠ“å–
            test_urls = self.test_url_crawling()
            if not test_urls:
                print("âŒ URL æŠ“å–å¤±è´¥ï¼Œç»ˆæ­¢æµ‹è¯•")
                return False
            
            # 2. æµ‹è¯•å†…å®¹æŠ“å–
            scraped_contents = self.test_content_scraping(test_urls)
            if not scraped_contents:
                print("âŒ å†…å®¹æŠ“å–å¤±è´¥ï¼Œç»ˆæ­¢æµ‹è¯•")
                return False
            
            # 3. æµ‹è¯•æ–‡æ¡£åˆ†ç±»
            classified_docs = self.test_document_classification(scraped_contents)
            if not classified_docs:
                print("âŒ æ–‡æ¡£åˆ†ç±»å¤±è´¥ï¼Œç»ˆæ­¢æµ‹è¯•")
                return False
            
            # 4. æµ‹è¯•å…ƒæ•°æ®ç”Ÿæˆ
            final_docs = self.test_metadata_generation(classified_docs)
            if not final_docs:
                print("âŒ å…ƒæ•°æ®ç”Ÿæˆå¤±è´¥ï¼Œç»ˆæ­¢æµ‹è¯•")
                return False
            
            # 5. æµ‹è¯• Dify ä¸Šä¼ 
            upload_success = self.test_dify_upload(final_docs)
            
            # 6. ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š
            self.generate_test_report()
            
            return upload_success
            
        except Exception as e:
            print(f"âŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºç°å¼‚å¸¸: {e}")
            import traceback
            traceback.print_exc()
            return False
        
        finally:
            # æ¸…ç†èµ„æº
            if hasattr(self, 'content_scraper'):
                self.content_scraper.close()
    
    def generate_test_report(self):
        """ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š"""
        print("\n" + "=" * 80)
        print("ğŸ“Š å®Œæ•´æµæ°´çº¿æµ‹è¯•æŠ¥å‘Š")
        print("=" * 80)
        
        test_items = [
            ('URL æŠ“å–', 'url_crawling'),
            ('å†…å®¹æŠ“å–', 'content_scraping'),
            ('æ–‡æ¡£åˆ†ç±»', 'document_classification'),
            ('å…ƒæ•°æ®ç”Ÿæˆ', 'metadata_generation'),
            ('Dify ä¸Šä¼ ', 'dify_upload')
        ]
        
        passed_tests = 0
        total_tests = len(test_items)
        
        for test_name, test_key in test_items:
            result = self.test_results.get(test_key, {'success': False})
            status = "âœ… é€šè¿‡" if result['success'] else "âŒ å¤±è´¥"
            print(f"   {test_name}: {status}")
            
            if result['success']:
                passed_tests += 1
                
                # æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
                if test_key == 'url_crawling':
                    print(f"     - å‘ç° URL: {result['url_count']} ä¸ª")
                elif test_key == 'content_scraping':
                    print(f"     - æŠ“å–æˆåŠŸ: {result['scraped_count']} ç¯‡")
                elif test_key == 'document_classification':
                    print(f"     - åˆ†ç±»å®Œæˆ: {result['classified_count']} ç¯‡")
                elif test_key == 'dify_upload':
                    print(f"     - ä¸Šä¼ æ–‡æ¡£: {result.get('document_title', 'N/A')}")
            else:
                print(f"     - é”™è¯¯: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")
        
        print(f"\nğŸ¯ æµ‹è¯•æ€»ç»“: {passed_tests}/{total_tests} é€šè¿‡")
        
        if passed_tests == total_tests:
            print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼ç³»ç»Ÿå„æ¨¡å—åŠŸèƒ½æ­£å¸¸")
            print("\nğŸ’¡ éªŒè¯çš„åŠŸèƒ½:")
            print("  âœ… URL æŠ“å– - èƒ½å¤Ÿè·å– TKE æ–‡æ¡£é“¾æ¥")
            print("  âœ… å†…å®¹æŠ“å– - èƒ½å¤Ÿæå–æ–‡æ¡£æ ‡é¢˜å’Œå†…å®¹")
            print("  âœ… æ–‡æ¡£åˆ†ç±» - èƒ½å¤Ÿè‡ªåŠ¨åˆ¤æ–­æ–‡æ¡£ç±»å‹")
            print("  âœ… å…ƒæ•°æ®ç”Ÿæˆ - èƒ½å¤Ÿç”Ÿæˆå®Œæ•´çš„æ–‡æ¡£å…ƒæ•°æ®")
            print("  âœ… Dify ä¸Šä¼  - èƒ½å¤ŸæˆåŠŸä¸Šä¼ åˆ°çŸ¥è¯†åº“")
            print("\nğŸš€ ç³»ç»Ÿå·²å‡†å¤‡å¥½è¿›è¡Œç”Ÿäº§ç¯å¢ƒéƒ¨ç½²ï¼")
        else:
            print("âš ï¸ éƒ¨åˆ†æµ‹è¯•æœªé€šè¿‡ï¼Œè¯·æ£€æŸ¥ç›¸å…³æ¨¡å—")


def main():
    """ä¸»å‡½æ•°"""
    tester = CompletePipelineTest()
    success = tester.run_complete_test()
    return success


if __name__ == "__main__":
    success = main()
    exit(0 if success else 1)