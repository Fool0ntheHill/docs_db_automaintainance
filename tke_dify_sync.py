import os
import json
import hashlib
import time
import shutil
import requests
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
from urllib.parse import urljoin
from typing import Set, Dict, List, Optional, Tuple
from dataclasses import dataclass
from pathlib import Path
from enhanced_metadata_generator import EnhancedMetadataGenerator
from dify_sync_manager import DifySyncManager, KnowledgeBaseStrategy
from tke_logger import setup_logger, get_logger, LogLevel
from secure_temp_manager import setup_temp_manager, get_temp_manager


class ConfigurationError(Exception):
    """é…ç½®ç›¸å…³å¼‚å¸¸"""
    def __init__(self, message: str, suggestions: List[str] = None):
        super().__init__(message)
        self.suggestions = suggestions or []
    
    def __str__(self):
        msg = super().__str__()
        if self.suggestions:
            msg += "\n\nğŸ’¡ å»ºè®®ï¼š"
            for i, suggestion in enumerate(self.suggestions, 1):
                msg += f"\n  {i}. {suggestion}"
        return msg


@dataclass
class Config:
    """é…ç½®æ•°æ®ç±»"""
    dify_api_key: str
    dify_knowledge_base_ids: List[str]
    dify_api_base_url: str
    kb_strategy: str = "primary"
    request_timeout: int = 10
    retry_attempts: int = 3
    retry_delay: int = 1
    state_file: str = "crawl_state.json"
    log_file: str = "tke_sync.log"
    base_url: str = "https://cloud.tencent.com"
    start_url: str = "https://cloud.tencent.com/document/product/457"


class ConfigManager:
    """é…ç½®ç®¡ç†å™¨ï¼Œæ”¯æŒç¯å¢ƒå˜é‡å’Œ .env æ–‡ä»¶"""
    
    def __init__(self, env_file: str = ".env"):
        self.env_file = env_file
        self.config: Optional[Config] = None
    
    def load_config(self) -> Config:
        """åŠ è½½é…ç½®ï¼Œä¼˜å…ˆçº§ï¼šç¯å¢ƒå˜é‡ > .env æ–‡ä»¶"""
        try:
            # é¦–å…ˆå°è¯•ä» .env æ–‡ä»¶åŠ è½½
            self._load_env_file()
            
            # è·å–é…ç½®å€¼
            dify_api_key = self._get_config_value("DIFY_API_KEY")
            dify_kb_ids_str = self._get_config_value("DIFY_KNOWLEDGE_BASE_ID")
            dify_api_base_url = self._get_config_value("DIFY_API_BASE_URL")
            
            # æ£€æŸ¥å¿…éœ€é…ç½®å¹¶æ”¶é›†ç¼ºå¤±é¡¹
            missing_configs = []
            config_suggestions = []
            
            if not dify_api_key or dify_api_key.strip() == "":
                missing_configs.append("DIFY_API_KEY")
                config_suggestions.extend([
                    "ç™»å½• Dify æ§åˆ¶å° â†’ è®¾ç½® â†’ API Keys",
                    "åˆ›å»ºæ–°çš„ API Key å¹¶å¤åˆ¶åˆ°é…ç½®æ–‡ä»¶"
                ])
            
            if not dify_kb_ids_str or dify_kb_ids_str.strip() == "":
                missing_configs.append("DIFY_KNOWLEDGE_BASE_ID")
                config_suggestions.extend([
                    "è¿›å…¥ Dify çŸ¥è¯†åº“é¡µé¢",
                    "ä» URL ä¸­è·å–çŸ¥è¯†åº“ IDï¼ˆæ ¼å¼ï¼š8c6b8e3c-f69c-48ea-b34e-a71798c800edï¼‰"
                ])
            
            if not dify_api_base_url or dify_api_base_url.strip() == "":
                missing_configs.append("DIFY_API_BASE_URL")
                config_suggestions.append("è®¾ç½® Dify API åŸºç¡€ URLï¼ˆé€šå¸¸ä¸ºï¼šhttps://api.dify.ai/v1ï¼‰")
            
            # å¦‚æœæœ‰ç¼ºå¤±é…ç½®ï¼ŒæŠ›å‡ºè¯¦ç»†çš„é”™è¯¯ä¿¡æ¯
            if missing_configs:
                suggestions = [
                    f"æ£€æŸ¥ {self.env_file} æ–‡ä»¶æ˜¯å¦å­˜åœ¨",
                    "ç¡®ä¿é…ç½®æ–‡ä»¶ä½¿ç”¨æ­£ç¡®çš„æ ¼å¼ï¼šKEY=VALUE",
                    "å‚è€ƒé¡¹ç›®ä¸­çš„ .env.example æ–‡ä»¶"
                ] + config_suggestions + [
                    "è¿è¡Œ 'python test_config.py' éªŒè¯é…ç½®"
                ]
                
                raise ConfigurationError(
                    f"âŒ ç¼ºå°‘å¿…éœ€çš„é…ç½®é¡¹: {', '.join(missing_configs)}",
                    suggestions
                )
            
            # è§£æçŸ¥è¯†åº“ IDï¼ˆæ”¯æŒé€—å·åˆ†éš”çš„å¤šä¸ª IDï¼‰
            try:
                dify_kb_ids = self._parse_knowledge_base_ids(dify_kb_ids_str)
                if not dify_kb_ids:
                    raise ConfigurationError(
                        "âŒ çŸ¥è¯†åº“ ID è§£æå¤±è´¥ï¼šæœªæ‰¾åˆ°æœ‰æ•ˆçš„çŸ¥è¯†åº“ ID",
                        [
                            "æ£€æŸ¥ DIFY_KNOWLEDGE_BASE_ID çš„æ ¼å¼",
                            "å•ä¸ªçŸ¥è¯†åº“ï¼šDIFY_KNOWLEDGE_BASE_ID=8c6b8e3c-f69c-48ea-b34e-a71798c800ed",
                            "å¤šä¸ªçŸ¥è¯†åº“ï¼šDIFY_KNOWLEDGE_BASE_ID=kb1-id,kb2-id,kb3-id",
                            "ç¡®ä¿çŸ¥è¯†åº“ ID ä¸ä¸ºç©ºä¸”æ ¼å¼æ­£ç¡®"
                        ]
                    )
            except Exception as e:
                if isinstance(e, ConfigurationError):
                    raise
                raise ConfigurationError(
                    f"âŒ çŸ¥è¯†åº“ ID è§£æé”™è¯¯: {e}",
                    [
                        "æ£€æŸ¥çŸ¥è¯†åº“ ID æ ¼å¼æ˜¯å¦æ­£ç¡®",
                        "ç¡®ä¿æ²¡æœ‰å¤šä½™çš„ç©ºæ ¼æˆ–ç‰¹æ®Šå­—ç¬¦",
                        "å¤šä¸ªçŸ¥è¯†åº“ ID ç”¨è‹±æ–‡é€—å·åˆ†éš”"
                    ]
                )
            
            # éªŒè¯æ•°å€¼é…ç½®
            try:
                request_timeout = int(self._get_config_value("REQUEST_TIMEOUT", "10"))
                retry_attempts = int(self._get_config_value("RETRY_ATTEMPTS", "3"))
                retry_delay = int(self._get_config_value("RETRY_DELAY", "1"))
                
                if request_timeout <= 0:
                    raise ValueError("REQUEST_TIMEOUT å¿…é¡»å¤§äº 0")
                if retry_attempts < 0:
                    raise ValueError("RETRY_ATTEMPTS ä¸èƒ½ä¸ºè´Ÿæ•°")
                if retry_delay < 0:
                    raise ValueError("RETRY_DELAY ä¸èƒ½ä¸ºè´Ÿæ•°")
                    
            except ValueError as e:
                raise ConfigurationError(
                    f"âŒ æ•°å€¼é…ç½®é”™è¯¯: {e}",
                    [
                        "REQUEST_TIMEOUT: è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰ï¼Œå»ºè®®å€¼ï¼š10-60",
                        "RETRY_ATTEMPTS: é‡è¯•æ¬¡æ•°ï¼Œå»ºè®®å€¼ï¼š1-5",
                        "RETRY_DELAY: é‡è¯•å»¶è¿Ÿï¼ˆç§’ï¼‰ï¼Œå»ºè®®å€¼ï¼š1-10",
                        "ç¡®ä¿æ‰€æœ‰æ•°å€¼é…ç½®éƒ½æ˜¯æ­£æ•´æ•°"
                    ]
                )
            
            # éªŒè¯ç­–ç•¥é…ç½®
            kb_strategy = self._get_config_value("KB_STRATEGY", "primary")
            valid_strategies = ["primary", "all", "round_robin"]
            if kb_strategy not in valid_strategies:
                raise ConfigurationError(
                    f"âŒ æ— æ•ˆçš„çŸ¥è¯†åº“ç­–ç•¥: {kb_strategy}",
                    [
                        f"æœ‰æ•ˆçš„ç­–ç•¥é€‰é¡¹: {', '.join(valid_strategies)}",
                        "primary: åªä½¿ç”¨ç¬¬ä¸€ä¸ªçŸ¥è¯†åº“ï¼ˆæ¨èï¼‰",
                        "all: åŒæ­¥åˆ°æ‰€æœ‰çŸ¥è¯†åº“",
                        "round_robin: è½®è¯¢åˆ†é…åˆ°ä¸åŒçŸ¥è¯†åº“"
                    ]
                )
            
            # åˆ›å»ºé…ç½®å¯¹è±¡
            self.config = Config(
                dify_api_key=dify_api_key,
                dify_knowledge_base_ids=dify_kb_ids,
                dify_api_base_url=dify_api_base_url,
                kb_strategy=kb_strategy,
                request_timeout=request_timeout,
                retry_attempts=retry_attempts,
                retry_delay=retry_delay,
                state_file=self._get_config_value("STATE_FILE", "crawl_state.json"),
                log_file=self._get_config_value("LOG_FILE", "tke_sync.log"),
                base_url=self._get_config_value("BASE_URL", "https://cloud.tencent.com"),
                start_url=self._get_config_value("START_URL", "https://cloud.tencent.com/document/product/457")
            )
            
            print(f"[é…ç½®] âœ… æˆåŠŸåŠ è½½é…ç½®ï¼ŒçŸ¥è¯†åº“æ•°é‡: {len(dify_kb_ids)}, ç­–ç•¥: {self.config.kb_strategy}")
            return self.config
            
        except ConfigurationError:
            raise  # é‡æ–°æŠ›å‡ºé…ç½®é”™è¯¯
        except Exception as e:
            # å¤„ç†å…¶ä»–æœªé¢„æœŸçš„é”™è¯¯
            suggestions = [
                f"æ£€æŸ¥ {self.env_file} æ–‡ä»¶æ˜¯å¦å­˜åœ¨ä¸”å¯è¯»",
                "ç¡®ä¿é…ç½®æ–‡ä»¶ä½¿ç”¨ UTF-8 ç¼–ç ",
                "æ£€æŸ¥é…ç½®æ–‡ä»¶ä¸­æ˜¯å¦æœ‰è¯­æ³•é”™è¯¯",
                "å°è¯•é‡æ–°åˆ›å»ºé…ç½®æ–‡ä»¶"
            ]
            raise ConfigurationError(f"âŒ é…ç½®åŠ è½½å¤±è´¥: {e}", suggestions)
    
    def _load_env_file(self):
        """åŠ è½½ .env æ–‡ä»¶"""
        env_path = Path(self.env_file)
        if not env_path.exists():
            # é…ç½®æ–‡ä»¶ä¸å­˜åœ¨ï¼Œç»™å‡ºå‹å¥½æç¤º
            if self.env_file == ".env":
                suggestions = [
                    "åˆ›å»º .env é…ç½®æ–‡ä»¶",
                    "å¤åˆ¶ .env.example ä¸º .envï¼ˆå¦‚æœå­˜åœ¨ï¼‰",
                    "å‚è€ƒ README.md ä¸­çš„é…ç½®è¯´æ˜",
                    "è¿è¡Œ 'python test_config.py' è·å–é…ç½®å¸®åŠ©"
                ]
                raise ConfigurationError(f"âŒ é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {env_path}", suggestions)
            else:
                # è‡ªå®šä¹‰é…ç½®æ–‡ä»¶ä¸å­˜åœ¨
                raise ConfigurationError(
                    f"âŒ æŒ‡å®šçš„é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {env_path}",
                    [
                        "æ£€æŸ¥æ–‡ä»¶è·¯å¾„æ˜¯å¦æ­£ç¡®",
                        "ç¡®ä¿æ–‡ä»¶å­˜åœ¨ä¸”å¯è¯»",
                        "æ£€æŸ¥æ–‡ä»¶åæ‹¼å†™"
                    ]
                )
        
        try:
            line_count = 0
            config_count = 0
            
            with open(env_path, 'r', encoding='utf-8') as f:
                for line_num, line in enumerate(f, 1):
                    line_count += 1
                    original_line = line
                    line = line.strip()
                    
                    # è·³è¿‡ç©ºè¡Œå’Œæ³¨é‡Š
                    if not line or line.startswith('#'):
                        continue
                    
                    # æ£€æŸ¥æ˜¯å¦åŒ…å«ç­‰å·
                    if '=' not in line:
                        print(f"[é…ç½®] âš ï¸ ç¬¬ {line_num} è¡Œæ ¼å¼å¯èƒ½æœ‰è¯¯: {original_line.strip()}")
                        continue
                    
                    try:
                        key, value = line.split('=', 1)
                        key = key.strip()
                        value = value.strip().strip('"').strip("'")
                        
                        # æ£€æŸ¥é”®åæ˜¯å¦æœ‰æ•ˆ
                        if not key:
                            print(f"[é…ç½®] âš ï¸ ç¬¬ {line_num} è¡Œï¼šé…ç½®é¡¹åç§°ä¸ºç©º")
                            continue
                        
                        # æ£€æŸ¥å€¼é•¿åº¦ï¼ˆWindows ç¯å¢ƒå˜é‡é™åˆ¶ï¼‰
                        if len(value) > 32000:
                            print(f"[é…ç½®] âš ï¸ é…ç½®é¡¹ {key} çš„å€¼è¿‡é•¿ ({len(value)} å­—ç¬¦)ï¼Œå¯èƒ½å¯¼è‡´é—®é¢˜")
                            value = value[:32000]  # æˆªæ–­
                        
                        # é…ç½®æ–‡ä»¶ä¸­çš„å€¼ä¼˜å…ˆäºç¯å¢ƒå˜é‡
                        os.environ[key] = value
                        config_count += 1
                        
                    except Exception as e:
                        print(f"[é…ç½®] âš ï¸ ç¬¬ {line_num} è¡Œè§£æå¤±è´¥: {e}")
                        continue
            
            print(f"[é…ç½®] âœ… å·²åŠ è½½é…ç½®æ–‡ä»¶: {env_path} ({config_count} ä¸ªé…ç½®é¡¹)")
            
            # è®¾ç½®é€‚å½“çš„æ–‡ä»¶æƒé™
            try:
                os.chmod(env_path, 0o600)
            except:
                pass  # æƒé™è®¾ç½®å¤±è´¥ä¸å½±å“åŠŸèƒ½
                
        except UnicodeDecodeError as e:
            raise ConfigurationError(
                f"âŒ é…ç½®æ–‡ä»¶ç¼–ç é”™è¯¯: {e}",
                [
                    "ç¡®ä¿é…ç½®æ–‡ä»¶ä½¿ç”¨ UTF-8 ç¼–ç ä¿å­˜",
                    "æ£€æŸ¥æ–‡ä»¶ä¸­æ˜¯å¦æœ‰ç‰¹æ®Šå­—ç¬¦",
                    "å°è¯•ç”¨æ–‡æœ¬ç¼–è¾‘å™¨é‡æ–°ä¿å­˜æ–‡ä»¶"
                ]
            )
        except PermissionError:
            raise ConfigurationError(
                f"âŒ æ— æƒé™è¯»å–é…ç½®æ–‡ä»¶: {env_path}",
                [
                    "æ£€æŸ¥æ–‡ä»¶æƒé™è®¾ç½®",
                    "ç¡®ä¿å½“å‰ç”¨æˆ·æœ‰è¯»å–æƒé™",
                    "å°è¯•ä»¥ç®¡ç†å‘˜èº«ä»½è¿è¡Œ"
                ]
            )
        except Exception as e:
            raise ConfigurationError(
                f"âŒ è¯»å–é…ç½®æ–‡ä»¶å¤±è´¥: {e}",
                [
                    "æ£€æŸ¥æ–‡ä»¶æ˜¯å¦è¢«å…¶ä»–ç¨‹åºå ç”¨",
                    "ç¡®ä¿æ–‡ä»¶æ²¡æœ‰æŸå",
                    "å°è¯•é‡æ–°åˆ›å»ºé…ç½®æ–‡ä»¶"
                ]
            )
    
    def _get_config_value(self, key: str, default: str = None) -> str:
        """è·å–é…ç½®å€¼ï¼Œä¼˜å…ˆä»ç¯å¢ƒå˜é‡è·å–"""
        return os.environ.get(key, default)
    
    def _parse_knowledge_base_ids(self, kb_ids_str: str) -> List[str]:
        """è§£æçŸ¥è¯†åº“ ID é…ç½®ï¼ˆæ”¯æŒå•ä¸ªæˆ–å¤šä¸ªï¼‰"""
        if not kb_ids_str:
            return []
        
        # æ”¯æŒé€—å·åˆ†éš”çš„å¤šä¸ª ID
        ids = [kb_id.strip() for kb_id in kb_ids_str.split(',') if kb_id.strip()]
        return ids
    
    def get_config(self) -> Config:
        """è·å–é…ç½®å¯¹è±¡"""
        if self.config is None:
            self.config = self.load_config()
        return self.config
    
    def validate_config(self) -> bool:
        """éªŒè¯é…ç½®æ˜¯å¦æœ‰æ•ˆ"""
        try:
            config = self.get_config()
            
            # éªŒè¯å¿…éœ€å­—æ®µ
            if not config.dify_api_key or config.dify_api_key == "sk-YOUR_DIFY_API_KEY" or config.dify_api_key.strip() == "":
                print("[é…ç½®] é”™è¯¯ï¼šDIFY_API_KEY æœªæ­£ç¡®è®¾ç½®æˆ–ä¸ºç©º")
                return False
            
            if not config.dify_knowledge_base_ids or len(config.dify_knowledge_base_ids) == 0:
                print("[é…ç½®] é”™è¯¯ï¼šDIFY_KNOWLEDGE_BASE_ID æœªæ­£ç¡®è®¾ç½®æˆ–ä¸ºç©º")
                return False
            
            # æ£€æŸ¥çŸ¥è¯†åº“IDæ˜¯å¦ä¸ºç©º
            for kb_id in config.dify_knowledge_base_ids:
                if not kb_id or kb_id.strip() == "":
                    print("[é…ç½®] é”™è¯¯ï¼šå‘ç°ç©ºçš„çŸ¥è¯†åº“ID")
                    return False
            
            if not config.dify_api_base_url or "your-dify-domain.com" in config.dify_api_base_url or config.dify_api_base_url.strip() == "":
                print("[é…ç½®] é”™è¯¯ï¼šDIFY_API_BASE_URL æœªæ­£ç¡®è®¾ç½®æˆ–ä¸ºç©º")
                return False
            
            # éªŒè¯ç­–ç•¥
            valid_strategies = ["primary", "all", "round_robin"]
            if config.kb_strategy not in valid_strategies:
                print(f"[é…ç½®] é”™è¯¯ï¼šKB_STRATEGY å¿…é¡»æ˜¯ {valid_strategies} ä¹‹ä¸€")
                return False
            
            # éªŒè¯æ•°å€¼é…ç½®
            if config.request_timeout <= 0:
                print("[é…ç½®] é”™è¯¯ï¼šREQUEST_TIMEOUT å¿…é¡»å¤§äº0")
                return False
                
            if config.retry_attempts < 0:
                print("[é…ç½®] é”™è¯¯ï¼šRETRY_ATTEMPTS ä¸èƒ½ä¸ºè´Ÿæ•°")
                return False
            
            print("[é…ç½®] é…ç½®éªŒè¯é€šè¿‡")
            return True
            
        except Exception as e:
            print(f"[é…ç½®] é…ç½®éªŒè¯å¤±è´¥: {e}")
            return False


def get_all_doc_urls(start_url: str, base_url: str = "https://cloud.tencent.com") -> Set[str]:
    """
    [ä»»åŠ¡ 1] æŠ“å–æ‰€æœ‰ TKE æ–‡æ¡£ URL (å·²éªŒè¯çš„ V7 é€»è¾‘)
    !!! Kiro æ³¨æ„ï¼šè¯·å‹¿ä¿®æ”¹æ­¤å‡½æ•°çš„å†…éƒ¨é€»è¾‘ !!!
    """
    print("[ä»»åŠ¡ 1] æ­£åœ¨å¯åŠ¨ Selenium (æœ‰å¤´æ¨¡å¼)...")
    options = webdriver.ChromeOptions()
    service = Service(ChromeDriverManager().install())
    driver = webdriver.Chrome(service=service, options=options)

    print(f"[ä»»åŠ¡ 1] æ­£åœ¨è®¿é—®: {start_url}")

    try:
        driver.get(start_url)
        
        print("[ä»»åŠ¡ 1] é¡µé¢åŠ è½½ã€‚ç¡¬æ€§ç­‰å¾… 5 ç§’é’Ÿï¼Œè®©æ‰€æœ‰ JS èµ„æºå…ˆç”Ÿæ•ˆ...")
        time.sleep(5) 

        # --- V7 æ ¸å¿ƒé€»è¾‘ï¼šä½¿ç”¨æœ€ç²¾ç¡®çš„é€‰æ‹©å™¨ ---
        
        # æ ¹å®¹å™¨
        nav_container_selector = "div.rno-column-aside-bd-2"
        
        # é€šç”¨é€‰æ‹©å™¨ (åŒ¹é… div å’Œ li)
        expandable_link_selector_text = (
            ".J-expandable:not(.active) "
            "a[href='javascript:void 0;']"
        )
        
        first_link_inside_nav = (By.CSS_SELECTOR, 
                                 f"{nav_container_selector} {expandable_link_selector_text}")
        
        print(f"[ä»»åŠ¡ 1] æ­£åœ¨ç­‰å¾… TKE èœå•å†…çš„ç¬¬ä¸€ä¸ª'æœªå±•å¼€'é“¾æ¥å˜ä¸ºå¯ç‚¹å‡»çŠ¶æ€...")
        
        WebDriverWait(driver, 20).until(
            EC.element_to_be_clickable(first_link_inside_nav)
        )
        print("[ä»»åŠ¡ 1] TKE èœå•å†…å®¹å·²åŠ è½½å¹¶å¯ç‚¹å‡»ã€‚å¼€å§‹è¿­ä»£å±•å¼€æ‰€æœ‰å­èœå•...")

        while True:
            links_selector = (By.CSS_SELECTOR, 
                              f"{nav_container_selector} {expandable_link_selector_text}")
            
            try:
                links_to_expand = driver.find_elements(*links_selector)
                
                if not links_to_expand:
                    print("[ä»»åŠ¡ 1] æ‰¾ä¸åˆ°æ›´å¤š *æœªå±•å¼€* çš„èœå•ã€‚èœå•å·²å®Œå…¨å±•å¼€ã€‚")
                    break
                
                print(f"[ä»»åŠ¡ 1] å‘ç° {len(links_to_expand)} ä¸ª *æœªå±•å¼€* é¡¹ã€‚æ­£åœ¨ç‚¹å‡»ç¬¬ä¸€ä¸ª...")
                
                link_to_click = links_to_expand[0]
                driver.execute_script("arguments[0].click();", link_to_click)
                time.sleep(0.3) 

            except Exception as e:
                print(f"[ä»»åŠ¡ 1] å±•å¼€æ—¶é‡åˆ°ä¸€ä¸ªä¸´æ—¶é”™è¯¯ (æ­£å¸¸ç°è±¡ï¼Œå¯å¿½ç•¥): {e}")
                time.sleep(0.5)

        print("[ä»»åŠ¡ 1] èœå•å±•å¼€å®Œæ¯•ã€‚æ­£åœ¨è§£ææ‰€æœ‰ URL...")
        
        page_source = driver.page_source
        soup = BeautifulSoup(page_source, 'html.parser')
        
        doc_urls: Set[str] = set()
        
        nav_wrapper = soup.find('div', class_='rno-column-aside-bd-2')
        if not nav_wrapper:
            print("[ä»»åŠ¡ 1] è­¦å‘Šï¼šæ‰¾ä¸åˆ° 'rno-column-aside-bd-2' å®¹å™¨ã€‚å°†ä»æ•´ä¸ªé¡µé¢è§£æã€‚")
            nav_wrapper = soup 
        
        for a_tag in nav_wrapper.find_all('a'):
            url = a_tag.get('data-link') or a_tag.get('href')
            
            if url and url.startswith('/document/product/457'):
                full_url = urljoin(base_url, url)
                doc_urls.add(full_url)

        return doc_urls

    finally:
        driver.quit()
        print("[ä»»åŠ¡ 1] Selenium å·²å…³é—­ã€‚")


class ContentScraper:
    """å¢å¼ºçš„å†…å®¹æŠ“å–å™¨ï¼Œå…·æœ‰å¥å£®çš„é”™è¯¯å¤„ç†"""
    
    def __init__(self, config: Config):
        self.config = config
        self.session = requests.Session()
        # è®¾ç½®ä¼šè¯çš„é»˜è®¤é…ç½®
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })
        # è®¾ç½®è¿æ¥æ± 
        adapter = requests.adapters.HTTPAdapter(
            pool_connections=10,
            pool_maxsize=20,
            max_retries=0  # æˆ‘ä»¬è‡ªå·±å¤„ç†é‡è¯•
        )
        self.session.mount('http://', adapter)
        self.session.mount('https://', adapter)
    
    def scrape_content(self, url: str) -> Optional[str]:
        """
        æŠ“å–å¹¶æ¸…æ´—æŒ‡å®š URL çš„æ ¸å¿ƒå†…å®¹
        
        Args:
            url: è¦æŠ“å–çš„ URL
            
        Returns:
            str: æ¸…æ´—åçš„å†…å®¹ï¼Œå¤±è´¥æ—¶è¿”å› None
        """
        for attempt in range(self.config.retry_attempts):
            try:
                return self._attempt_scrape(url, attempt + 1)
            except requests.exceptions.Timeout:
                print(f"[å†…å®¹æŠ“å–] è¶…æ—¶ (å°è¯• {attempt + 1}/{self.config.retry_attempts}): {url}")
                if attempt < self.config.retry_attempts - 1:
                    time.sleep(self.config.retry_delay * (attempt + 1))
                continue
            except requests.exceptions.ConnectionError as e:
                print(f"[å†…å®¹æŠ“å–] è¿æ¥é”™è¯¯ (å°è¯• {attempt + 1}/{self.config.retry_attempts}): {url} - {e}")
                if attempt < self.config.retry_attempts - 1:
                    time.sleep(self.config.retry_delay * (attempt + 1))
                continue
            except requests.exceptions.HTTPError as e:
                status_code = e.response.status_code if e.response else 'Unknown'
                print(f"[å†…å®¹æŠ“å–] HTTP é”™è¯¯ {status_code}: {url} - {e}")
                
                # å¯¹äºæŸäº›é”™è¯¯ç ä¸é‡è¯•
                if status_code in [404, 403, 401, 410]:
                    print(f"[å†…å®¹æŠ“å–] ä¸å¯é‡è¯•çš„é”™è¯¯ï¼Œè·³è¿‡: {url}")
                    break
                
                # å¯¹äºæœåŠ¡å™¨é”™è¯¯ï¼Œé‡è¯•
                if attempt < self.config.retry_attempts - 1 and status_code >= 500:
                    time.sleep(self.config.retry_delay * (attempt + 1))
                    continue
                break
            except Exception as e:
                print(f"[å†…å®¹æŠ“å–] æœªé¢„æœŸé”™è¯¯ (å°è¯• {attempt + 1}/{self.config.retry_attempts}): {url} - {type(e).__name__}: {e}")
                if attempt < self.config.retry_attempts - 1:
                    time.sleep(self.config.retry_delay * (attempt + 1))
                continue
        
        print(f"[å†…å®¹æŠ“å–] æ‰€æœ‰å°è¯•å¤±è´¥ï¼Œè·³è¿‡: {url}")
        return None
    
    def _attempt_scrape(self, url: str, attempt_num: int) -> Optional[str]:
        """
        å•æ¬¡æŠ“å–å°è¯•
        
        Args:
            url: è¦æŠ“å–çš„ URL
            attempt_num: å°è¯•æ¬¡æ•°
            
        Returns:
            str: æ¸…æ´—åçš„å†…å®¹
            
        Raises:
            å„ç§ requests å¼‚å¸¸
        """
        print(f"[å†…å®¹æŠ“å–] å¼€å§‹æŠ“å– (å°è¯• {attempt_num}): {url}")
        
        # å‘é€è¯·æ±‚
        response = self.session.get(
            url, 
            timeout=self.config.request_timeout,
            allow_redirects=True
        )
        response.raise_for_status()
        
        # æ£€æŸ¥å“åº”å†…å®¹ç±»å‹
        content_type = response.headers.get('content-type', '').lower()
        if 'text/html' not in content_type:
            print(f"[å†…å®¹æŠ“å–] è­¦å‘Šï¼šé HTML å†…å®¹ç±»å‹ {content_type}: {url}")
        
        # æ£€æŸ¥å“åº”å¤§å°
        content_length = len(response.content)
        if content_length == 0:
            print(f"[å†…å®¹æŠ“å–] è­¦å‘Šï¼šå“åº”å†…å®¹ä¸ºç©º: {url}")
            return None
        
        if content_length > 10 * 1024 * 1024:  # 10MB
            print(f"[å†…å®¹æŠ“å–] è­¦å‘Šï¼šå“åº”å†…å®¹è¿‡å¤§ ({content_length} bytes): {url}")
        
        # è§£æ HTML
        try:
            soup = BeautifulSoup(response.text, 'html.parser')
        except Exception as e:
            print(f"[å†…å®¹æŠ“å–] HTML è§£æå¤±è´¥: {url} - {e}")
            return None
        
        # æå–å†…å®¹å’Œæ ‡é¢˜
        content = self._extract_content(soup, url)
        title = self._extract_title(soup, url)
        
        if content:
            print(f"[å†…å®¹æŠ“å–] æˆåŠŸæå–å†…å®¹ ({len(content)} å­—ç¬¦): {url}")
            print(f"[æ ‡é¢˜æå–] æå–æ ‡é¢˜: {title}")
            # å°†æ ‡é¢˜å’Œå†…å®¹ç»„åˆè¿”å›ï¼Œä½¿ç”¨ç‰¹æ®Šåˆ†éš”ç¬¦
            return f"TITLE:{title}\nCONTENT:{content}"
        else:
            print(f"[å†…å®¹æŠ“å–] æœªæ‰¾åˆ°æœ‰æ•ˆå†…å®¹: {url}")
            return None
    
    def _extract_content(self, soup: BeautifulSoup, url: str) -> Optional[str]:
        """
        ä» BeautifulSoup å¯¹è±¡ä¸­æå–å†…å®¹
        
        Args:
            soup: BeautifulSoup å¯¹è±¡
            url: åŸå§‹ URLï¼ˆç”¨äºæ—¥å¿—ï¼‰
            
        Returns:
            str: æå–çš„å†…å®¹ï¼Œå¤±è´¥æ—¶è¿”å› None
        """
        # ä¸»è¦é€‰æ‹©å™¨
        primary_selector = 'div.content-layout-container'
        
        # å¤‡ç”¨é€‰æ‹©å™¨ï¼ˆæŒ‰ä¼˜å…ˆçº§æ’åºï¼‰
        fallback_selectors = [
            'div.content-container',
            'div.main-content',
            'article',
            'div.content',
            'main',
            'div#content'
        ]
        
        # å°è¯•ä¸»è¦é€‰æ‹©å™¨
        content_div = soup.find('div', class_='content-layout-container')
        if content_div:
            content = self._clean_content(content_div.get_text(separator='\n', strip=True))
            if content and len(content.strip()) > 50:  # è‡³å°‘ 50 ä¸ªå­—ç¬¦
                return content
            else:
                print(f"[å†…å®¹æŠ“å–] ä¸»é€‰æ‹©å™¨æ‰¾åˆ°å†…å®¹ä½†å¤ªçŸ­: {url}")
        
        # å°è¯•å¤‡ç”¨é€‰æ‹©å™¨
        for selector in fallback_selectors:
            try:
                if '.' in selector:
                    class_name = selector.split('.')[1]
                    element = soup.find('div', class_=class_name)
                elif '#' in selector:
                    id_name = selector.split('#')[1]
                    element = soup.find('div', id=id_name)
                else:
                    element = soup.find(selector)
                
                if element:
                    content = self._clean_content(element.get_text(separator='\n', strip=True))
                    if content and len(content.strip()) > 50:
                        print(f"[å†…å®¹æŠ“å–] ä½¿ç”¨å¤‡ç”¨é€‰æ‹©å™¨ {selector}: {url}")
                        return content
            except Exception as e:
                print(f"[å†…å®¹æŠ“å–] å¤‡ç”¨é€‰æ‹©å™¨ {selector} å¤±è´¥: {url} - {e}")
                continue
        
        # æœ€åå°è¯•ï¼šæå– body å†…å®¹
        try:
            body = soup.find('body')
            if body:
                # ç§»é™¤è„šæœ¬å’Œæ ·å¼æ ‡ç­¾
                for script in body(["script", "style", "nav", "header", "footer"]):
                    script.decompose()
                
                content = self._clean_content(body.get_text(separator='\n', strip=True))
                if content and len(content.strip()) > 100:  # body å†…å®¹è¦æ±‚æ›´é•¿
                    print(f"[å†…å®¹æŠ“å–] ä½¿ç”¨ body å†…å®¹ä½œä¸ºåå¤‡: {url}")
                    return content
        except Exception as e:
            print(f"[å†…å®¹æŠ“å–] body å†…å®¹æå–å¤±è´¥: {url} - {e}")
        
        return None
    
    def _clean_content(self, content: str) -> str:
        """
        æ¸…æ´—æå–çš„å†…å®¹
        
        Args:
            content: åŸå§‹å†…å®¹
            
        Returns:
            str: æ¸…æ´—åçš„å†…å®¹
        """
        if not content:
            return ""
        
        # ç§»é™¤å¤šä½™çš„ç©ºç™½å­—ç¬¦
        lines = []
        for line in content.split('\n'):
            line = line.strip()
            if line:  # è·³è¿‡ç©ºè¡Œ
                lines.append(line)
        
        # åˆå¹¶è¿ç»­çš„çŸ­è¡Œï¼ˆå¯èƒ½æ˜¯è¢«é”™è¯¯åˆ†å‰²çš„ï¼‰
        merged_lines = []
        i = 0
        while i < len(lines):
            current_line = lines[i]
            
            # å¦‚æœå½“å‰è¡Œå¾ˆçŸ­ä¸”ä¸‹ä¸€è¡Œå­˜åœ¨ï¼Œå°è¯•åˆå¹¶
            while (i + 1 < len(lines) and 
                   len(current_line) < 100 and 
                   not current_line.endswith(('ã€‚', 'ï¼', 'ï¼Ÿ', '.', '!', '?', ':', 'ï¼š'))):
                current_line += ' ' + lines[i + 1]
                i += 1
            
            merged_lines.append(current_line)
            i += 1
        
        return '\n'.join(merged_lines)
    
    def _extract_title(self, soup: BeautifulSoup, url: str) -> str:
        """
        ä»é¡µé¢ä¸­æå–æ ‡é¢˜
        
        Args:
            soup: BeautifulSoup å¯¹è±¡
            url: åŸå§‹ URLï¼ˆç”¨äºæ—¥å¿—ï¼‰
            
        Returns:
            str: æå–çš„æ ‡é¢˜
        """
        # å°è¯•å¤šç§æ ‡é¢˜é€‰æ‹©å™¨ï¼ŒæŒ‰ä¼˜å…ˆçº§æ’åº
        title_selectors = [
            'h1',                                    # ä¸»æ ‡é¢˜
            '.content-layout-container h1',          # å†…å®¹åŒºåŸŸçš„ä¸»æ ‡é¢˜
            '.content-layout-container h2',          # å†…å®¹åŒºåŸŸçš„å‰¯æ ‡é¢˜
            'title',                                 # é¡µé¢æ ‡é¢˜
            '.page-title',                           # é¡µé¢æ ‡é¢˜ç±»
            '.article-title',                        # æ–‡ç« æ ‡é¢˜ç±»
            '.doc-title'                             # æ–‡æ¡£æ ‡é¢˜ç±»
        ]
        
        for selector in title_selectors:
            try:
                element = soup.select_one(selector)
                if element:
                    title = element.get_text(strip=True)
                    if title and len(title) > 0:
                        # æ¸…ç†æ ‡é¢˜
                        title = title.replace('\n', ' ').replace('\r', ' ')
                        title = ' '.join(title.split())  # åˆå¹¶å¤šä¸ªç©ºæ ¼
                        if len(title) <= 200:  # æ ‡é¢˜é•¿åº¦é™åˆ¶
                            print(f"[æ ‡é¢˜æå–] ä½¿ç”¨é€‰æ‹©å™¨ {selector} æå–æ ‡é¢˜: {title}")
                            return title
            except Exception as e:
                print(f"[æ ‡é¢˜æå–] é€‰æ‹©å™¨ {selector} å¤±è´¥: {e}")
                continue
        
        # å¦‚æœéƒ½å¤±è´¥äº†ï¼Œä» URL ç”Ÿæˆæ ‡é¢˜
        try:
            from urllib.parse import urlparse
            parsed = urlparse(url)
            path_parts = [part for part in parsed.path.split('/') if part]
            
            if len(path_parts) >= 3 and path_parts[0] == 'document' and path_parts[1] == 'product':
                product_id = path_parts[2]
                doc_id = path_parts[3] if len(path_parts) >= 4 else 'unknown'
                title = f"TKE æ–‡æ¡£ - {product_id} - {doc_id}"
            else:
                title = f"TKE æ–‡æ¡£ - {url.split('/')[-1]}"
            
            print(f"[æ ‡é¢˜æå–] ä½¿ç”¨ URL ç”Ÿæˆæ ‡é¢˜: {title}")
            return title
            
        except Exception as e:
            print(f"[æ ‡é¢˜æå–] URL è§£æå¤±è´¥: {e}")
            return f"TKE æ–‡æ¡£ - {url}"
    
    def close(self):
        """å…³é—­ä¼šè¯"""
        if self.session:
            self.session.close()


def scrape_content(url: str, config: Config = None) -> Optional[str]:
    """
    å‘åå…¼å®¹çš„å†…å®¹æŠ“å–å‡½æ•°
    
    Args:
        url: è¦æŠ“å–çš„ URL
        config: é…ç½®å¯¹è±¡ï¼Œå¦‚æœä¸º None åˆ™ä½¿ç”¨é»˜è®¤é…ç½®
        
    Returns:
        str: æŠ“å–çš„å†…å®¹ï¼Œå¤±è´¥æ—¶è¿”å› None
    """
    if config is None:
        # åˆ›å»ºé»˜è®¤é…ç½®
        config = Config(
            dify_api_key="dummy",
            dify_knowledge_base_ids=["dummy"],
            dify_api_base_url="dummy"
        )
    
    scraper = ContentScraper(config)
    try:
        return scraper.scrape_content(url)
    finally:
        scraper.close()

def get_content_hash(content: str) -> str:
    """[ä»»åŠ¡ 3 è¾…åŠ©] è®¡ç®—å†…å®¹çš„ MD5 å“ˆå¸Œå€¼"""
    return hashlib.md5(content.encode('utf-8')).hexdigest()

class StateManager:
    """åŸå­æ€§çŠ¶æ€ç®¡ç†å™¨ï¼Œç¡®ä¿çŠ¶æ€æ–‡ä»¶çš„å®Œæ•´æ€§å’Œä¸€è‡´æ€§"""
    
    def __init__(self, state_file: str):
        self.state_file = state_file
        self.backup_file = f"{state_file}.backup"
        self.temp_file = f"{state_file}.tmp"
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'load_attempts': 0,
            'load_successes': 0,
            'save_attempts': 0,
            'save_successes': 0,
            'corruption_recoveries': 0,
            'backup_recoveries': 0
        }
    
    def load_state(self) -> Dict[str, str]:
        """
        åŠ è½½çŠ¶æ€æ–‡ä»¶ï¼Œæ”¯æŒæŸåæ£€æµ‹å’Œè‡ªåŠ¨æ¢å¤
        
        Returns:
            çŠ¶æ€å­—å…¸ï¼Œå¤±è´¥æ—¶è¿”å›ç©ºå­—å…¸
        """
        self.stats['load_attempts'] += 1
        
        # é¦–å…ˆå°è¯•åŠ è½½ä¸»çŠ¶æ€æ–‡ä»¶
        state = self._load_file(self.state_file)
        if state is not None:
            self.stats['load_successes'] += 1
            return state
        
        print(f"[ä»»åŠ¡ 3] ä¸»çŠ¶æ€æ–‡ä»¶æŸåæˆ–ä¸å­˜åœ¨: {self.state_file}")
        
        # å°è¯•ä»å¤‡ä»½æ–‡ä»¶æ¢å¤
        if os.path.exists(self.backup_file):
            print(f"[ä»»åŠ¡ 3] å°è¯•ä»å¤‡ä»½æ–‡ä»¶æ¢å¤: {self.backup_file}")
            backup_state = self._load_file(self.backup_file)
            if backup_state is not None:
                self.stats['backup_recoveries'] += 1
                self.stats['load_successes'] += 1
                print(f"[ä»»åŠ¡ 3] æˆåŠŸä»å¤‡ä»½æ¢å¤çŠ¶æ€ï¼ŒåŒ…å« {len(backup_state)} ä¸ªæ¡ç›®")
                
                # å°è¯•ä¿®å¤ä¸»çŠ¶æ€æ–‡ä»¶
                try:
                    self._atomic_save(self.state_file, backup_state)
                    print(f"[ä»»åŠ¡ 3] ä¸»çŠ¶æ€æ–‡ä»¶å·²ä¿®å¤")
                except Exception as e:
                    print(f"[ä»»åŠ¡ 3] è­¦å‘Šï¼šæ— æ³•ä¿®å¤ä¸»çŠ¶æ€æ–‡ä»¶ - {e}")
                
                return backup_state
        
        # æ‰€æœ‰æ¢å¤å°è¯•éƒ½å¤±è´¥ï¼Œè¿”å›ç©ºçŠ¶æ€
        print(f"[ä»»åŠ¡ 3] æ— æ³•æ¢å¤çŠ¶æ€ï¼Œå°†ä»ç©ºçŠ¶æ€å¼€å§‹")
        self.stats['corruption_recoveries'] += 1
        self.stats['load_successes'] += 1  # è¿”å›ç©ºçŠ¶æ€ä¹Ÿç®—æˆåŠŸ
        return {}
    
    def save_state(self, state: Dict[str, str]) -> bool:
        """
        åŸå­æ€§ä¿å­˜çŠ¶æ€æ–‡ä»¶
        
        Args:
            state: è¦ä¿å­˜çš„çŠ¶æ€å­—å…¸
            
        Returns:
            ä¿å­˜æ˜¯å¦æˆåŠŸ
        """
        self.stats['save_attempts'] += 1
        
        try:
            # åˆ›å»ºå¤‡ä»½ï¼ˆå¦‚æœä¸»æ–‡ä»¶å­˜åœ¨ä¸”æœ‰æ•ˆï¼‰
            if os.path.exists(self.state_file):
                current_state = self._load_file(self.state_file)
                if current_state is not None:
                    try:
                        shutil.copy2(self.state_file, self.backup_file)
                        print(f"[ä»»åŠ¡ 3] å·²åˆ›å»ºçŠ¶æ€æ–‡ä»¶å¤‡ä»½")
                    except Exception as e:
                        print(f"[ä»»åŠ¡ 3] è­¦å‘Šï¼šæ— æ³•åˆ›å»ºå¤‡ä»½ - {e}")
            
            # åŸå­æ€§ä¿å­˜åˆ°ä¸»æ–‡ä»¶
            success = self._atomic_save(self.state_file, state)
            if success:
                self.stats['save_successes'] += 1
                print(f"[ä»»åŠ¡ 3] çŠ¶æ€å·²ä¿å­˜ï¼ŒåŒ…å« {len(state)} ä¸ªæ¡ç›®")
                return True
            else:
                print(f"[ä»»åŠ¡ 3] çŠ¶æ€ä¿å­˜å¤±è´¥")
                return False
                
        except Exception as e:
            print(f"[ä»»åŠ¡ 3] çŠ¶æ€ä¿å­˜å¼‚å¸¸: {type(e).__name__}: {e}")
            return False
    
    def _load_file(self, file_path: str) -> Optional[Dict[str, str]]:
        """
        å®‰å…¨åŠ è½½ JSON æ–‡ä»¶
        
        Args:
            file_path: æ–‡ä»¶è·¯å¾„
            
        Returns:
            è§£æçš„å­—å…¸ï¼Œå¤±è´¥æ—¶è¿”å› None
        """
        if not os.path.exists(file_path):
            return None
        
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read().strip()
                if not content:
                    print(f"[ä»»åŠ¡ 3] æ–‡ä»¶ä¸ºç©º: {file_path}")
                    return None
                
                data = json.loads(content)
                if not isinstance(data, dict):
                    print(f"[ä»»åŠ¡ 3] æ–‡ä»¶æ ¼å¼é”™è¯¯ï¼Œä¸æ˜¯å­—å…¸: {file_path}")
                    return None
                
                # éªŒè¯æ‰€æœ‰é”®å€¼éƒ½æ˜¯å­—ç¬¦ä¸²
                for key, value in data.items():
                    if not isinstance(key, str) or not isinstance(value, str):
                        print(f"[ä»»åŠ¡ 3] çŠ¶æ€æ–‡ä»¶åŒ…å«éå­—ç¬¦ä¸²é”®å€¼å¯¹: {file_path}")
                        return None
                
                return data
                
        except json.JSONDecodeError as e:
            print(f"[ä»»åŠ¡ 3] JSON è§£æé”™è¯¯: {file_path} - {e}")
            return None
        except Exception as e:
            print(f"[ä»»åŠ¡ 3] æ–‡ä»¶è¯»å–é”™è¯¯: {file_path} - {type(e).__name__}: {e}")
            return None
    
    def _atomic_save(self, file_path: str, state: Dict[str, str]) -> bool:
        """
        åŸå­æ€§ä¿å­˜æ–‡ä»¶ï¼ˆå…ˆå†™ä¸´æ—¶æ–‡ä»¶ï¼Œå†é‡å‘½åï¼‰
        
        Args:
            file_path: ç›®æ ‡æ–‡ä»¶è·¯å¾„
            state: è¦ä¿å­˜çš„çŠ¶æ€
            
        Returns:
            ä¿å­˜æ˜¯å¦æˆåŠŸ
        """
        try:
            # ç¡®ä¿ç›®å½•å­˜åœ¨
            dir_path = os.path.dirname(file_path)
            if dir_path:  # åªæœ‰å½“ç›®å½•è·¯å¾„ä¸ä¸ºç©ºæ—¶æ‰åˆ›å»º
                os.makedirs(dir_path, exist_ok=True)
            
            # å†™å…¥ä¸´æ—¶æ–‡ä»¶
            with open(self.temp_file, 'w', encoding='utf-8') as f:
                json.dump(state, f, indent=2, ensure_ascii=False)
                f.flush()  # ç¡®ä¿æ•°æ®å†™å…¥ç£ç›˜
                os.fsync(f.fileno())  # å¼ºåˆ¶åŒæ­¥åˆ°ç£ç›˜
            
            # éªŒè¯ä¸´æ—¶æ–‡ä»¶
            temp_state = self._load_file(self.temp_file)
            if temp_state != state:
                print(f"[ä»»åŠ¡ 3] ä¸´æ—¶æ–‡ä»¶éªŒè¯å¤±è´¥")
                return False
            
            # åŸå­æ€§é‡å‘½å
            if os.name == 'nt':  # Windows
                # Windows ä¸æ”¯æŒåŸå­æ€§é‡å‘½ååˆ°å·²å­˜åœ¨çš„æ–‡ä»¶
                if os.path.exists(file_path):
                    os.remove(file_path)
                os.rename(self.temp_file, file_path)
            else:  # Unix/Linux
                os.rename(self.temp_file, file_path)
            
            return True
            
        except Exception as e:
            print(f"[ä»»åŠ¡ 3] åŸå­æ€§ä¿å­˜å¤±è´¥: {type(e).__name__}: {e}")
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            try:
                if os.path.exists(self.temp_file):
                    os.remove(self.temp_file)
            except:
                pass
            return False
    
    def cleanup_temp_files(self) -> None:
        """æ¸…ç†ä¸´æ—¶æ–‡ä»¶"""
        for temp_path in [self.temp_file]:
            try:
                if os.path.exists(temp_path):
                    os.remove(temp_path)
                    print(f"[ä»»åŠ¡ 3] å·²æ¸…ç†ä¸´æ—¶æ–‡ä»¶: {temp_path}")
            except Exception as e:
                print(f"[ä»»åŠ¡ 3] æ¸…ç†ä¸´æ—¶æ–‡ä»¶å¤±è´¥: {temp_path} - {e}")
    
    def get_stats(self) -> Dict[str, int]:
        """è·å–çŠ¶æ€ç®¡ç†ç»Ÿè®¡ä¿¡æ¯"""
        return self.stats.copy()
    
    def print_stats(self) -> None:
        """æ‰“å°çŠ¶æ€ç®¡ç†ç»Ÿè®¡ä¿¡æ¯"""
        print("\\n=== çŠ¶æ€ç®¡ç†ç»Ÿè®¡ ===")
        print(f"åŠ è½½å°è¯•: {self.stats['load_attempts']}")
        print(f"åŠ è½½æˆåŠŸ: {self.stats['load_successes']}")
        print(f"ä¿å­˜å°è¯•: {self.stats['save_attempts']}")
        print(f"ä¿å­˜æˆåŠŸ: {self.stats['save_successes']}")
        print(f"æŸåæ¢å¤: {self.stats['corruption_recoveries']}")
        print(f"å¤‡ä»½æ¢å¤: {self.stats['backup_recoveries']}")
        
        if self.stats['load_attempts'] > 0:
            load_rate = (self.stats['load_successes'] / self.stats['load_attempts']) * 100
            print(f"åŠ è½½æˆåŠŸç‡: {load_rate:.1f}%")
        
        if self.stats['save_attempts'] > 0:
            save_rate = (self.stats['save_successes'] / self.stats['save_attempts']) * 100
            print(f"ä¿å­˜æˆåŠŸç‡: {save_rate:.1f}%")
        
        print("==================\\n")


def load_state(file_path: str) -> Dict[str, str]:
    """
    å‘åå…¼å®¹çš„çŠ¶æ€åŠ è½½å‡½æ•°
    
    Args:
        file_path: çŠ¶æ€æ–‡ä»¶è·¯å¾„
        
    Returns:
        çŠ¶æ€å­—å…¸
    """
    manager = StateManager(file_path)
    return manager.load_state()


def save_state(file_path: str, state: Dict[str, str]) -> None:
    """
    å‘åå…¼å®¹çš„çŠ¶æ€ä¿å­˜å‡½æ•°
    
    Args:
        file_path: çŠ¶æ€æ–‡ä»¶è·¯å¾„
        state: çŠ¶æ€å­—å…¸
    """
    manager = StateManager(file_path)
    manager.save_state(state)

class TKEDifySync:
    """TKE æ–‡æ¡£åŒæ­¥å™¨ï¼Œä½¿ç”¨æ™ºèƒ½å“ˆå¸Œå¯¹æ¯”"""
    
    def __init__(self, config: Config):
        self.config = config
        self.dify_manager = DifySyncManager(config)
        self.metadata_generator = EnhancedMetadataGenerator()
    
    def sync_to_dify(self, url: str, content: str, metadata: Dict = None) -> bool:
        """
        å°†æ–‡æ¡£åŒæ­¥åˆ° Dify çŸ¥è¯†åº“ï¼ˆä½¿ç”¨æ™ºèƒ½å“ˆå¸Œå¯¹æ¯”ï¼‰
        
        Args:
            url: æ–‡æ¡£ URL
            content: æ–‡æ¡£å†…å®¹
            metadata: å…ƒæ•°æ®
            
        Returns:
            æ˜¯å¦æˆåŠŸ
        """
        print(f"[åŒæ­¥] å‡†å¤‡åŒæ­¥åˆ° Dify: {url}")
        
        try:
            # è§£ææ ‡é¢˜å’Œå†…å®¹
            if content.startswith("TITLE:") and "\nCONTENT:" in content:
                parts = content.split("\nCONTENT:", 1)
                title = parts[0].replace("TITLE:", "").strip()
                actual_content = parts[1].strip()
            else:
                # å¦‚æœæ²¡æœ‰æ ‡é¢˜åˆ†éš”ç¬¦ï¼Œä»URLç”Ÿæˆæ ‡é¢˜
                from urllib.parse import urlparse
                parsed = urlparse(url)
                path_parts = [part for part in parsed.path.split('/') if part]
                
                if len(path_parts) >= 4 and path_parts[0] == 'document' and path_parts[1] == 'product':
                    doc_id = path_parts[3]
                    title = f"TKE æ–‡æ¡£ - {doc_id}"
                else:
                    title = f"TKE æ–‡æ¡£ - {url.split('/')[-1]}"
                
                actual_content = content
            
            # å¦‚æœæ²¡æœ‰æä¾›å…ƒæ•°æ®ï¼Œç”Ÿæˆå…ƒæ•°æ®
            if metadata is None:
                metadata = self.metadata_generator.generate_metadata(url, actual_content)
            
            # ä½¿ç”¨ DifySyncManager çš„æ™ºèƒ½åŒæ­¥åŠŸèƒ½
            success = self.dify_manager.sync_document(url, f"TITLE:{title}\nCONTENT:{actual_content}", metadata)
            
            if success:
                print(f"[åŒæ­¥] âœ… æˆåŠŸï¼š{url} å·²åŒæ­¥åˆ° Difyï¼ˆä½¿ç”¨æ™ºèƒ½å“ˆå¸Œå¯¹æ¯”ï¼‰")
            else:
                print(f"[åŒæ­¥] âŒ å¤±è´¥ï¼š{url} åŒæ­¥å¤±è´¥")
            
            return success
            
        except Exception as e:
            print(f"[åŒæ­¥] âŒ å¼‚å¸¸ï¼š{url} åŒæ­¥æ—¶å‘ç”Ÿé”™è¯¯: {type(e).__name__}: {e}")
            return False
    
    def sync_documents(self, documents: List[Tuple[str, str, Dict]]) -> bool:
        """
        æ‰¹é‡åŒæ­¥æ–‡æ¡£
        
        Args:
            documents: æ–‡æ¡£åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ ä¸º (url, content, metadata)
            
        Returns:
            æ˜¯å¦å…¨éƒ¨æˆåŠŸ
        """
        print(f"[åŒæ­¥] å¼€å§‹æ‰¹é‡åŒæ­¥ {len(documents)} ä¸ªæ–‡æ¡£")
        
        success_count = 0
        total_count = len(documents)
        
        for i, (url, content, metadata) in enumerate(documents, 1):
            print(f"\n[åŒæ­¥] å¤„ç†æ–‡æ¡£ {i}/{total_count}: {url}")
            
            if self.sync_to_dify(url, content, metadata):
                success_count += 1
        
        print(f"\n[åŒæ­¥] æ‰¹é‡åŒæ­¥å®Œæˆ: {success_count}/{total_count} æˆåŠŸ")
        
        # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
        self.dify_manager.print_stats()
        
        return success_count == total_count
    
    def get_stats(self) -> Dict:
        """è·å–åŒæ­¥ç»Ÿè®¡ä¿¡æ¯"""
        return self.dify_manager.get_stats()


def sync_to_dify(url: str, content: str, config: Config, metadata: Dict = None) -> bool:
    """
    [ä»»åŠ¡ 4] å°†æ–°/å˜æ›´çš„æ–‡æ¡£åŒæ­¥åˆ° Dify çŸ¥è¯†åº“ã€‚
    Kiro ä»»åŠ¡ï¼šè¯·æ ¹æ® API æ–‡æ¡£é‡å†™æ­¤å‡½æ•°ï¼Œå®ç°â€œæ›´æ–°â€é€»è¾‘å¹¶å¤„ç†æ‰€æœ‰ API é”™è¯¯ã€‚
    """
    print(f"[ä»»åŠ¡ 4] å‡†å¤‡åŒæ­¥åˆ° Dify: {url}")
    
    # Kiro æ³¨æ„ï¼šä»¥ä¸‹æ˜¯â€œåˆ›å»ºâ€é€»è¾‘çš„è‰æ¡ˆï¼Œä½ éœ€è¦å®ç°â€œæ›´æ–°â€é€»è¾‘
    


    # å‡†å¤‡ API è¯·æ±‚
    # ä½¿ç”¨ç¬¬ä¸€ä¸ªçŸ¥è¯†åº“ IDï¼ˆprimary ç­–ç•¥ï¼‰
    kb_id = config.dify_knowledge_base_ids[0]
    api_url = f"{config.dify_api_base_url}/datasets/{kb_id}/document/create_by_text"
    headers = {"Authorization": f"Bearer {config.dify_api_key}", "Content-Type": "application/json"}
    
    # å‡†å¤‡è¯·æ±‚æ•°æ®
    document_name = url
    if metadata:
        # ä½¿ç”¨å…ƒæ•°æ®ç”Ÿæˆæ›´å¥½çš„æ–‡æ¡£åç§°
        doc_type = metadata.get('document_type', 'æ–‡æ¡£')
        difficulty = metadata.get('difficulty_level', '')
        product_id = metadata.get('product_id', '')
        
        if product_id:
            document_name = f"[{product_id}] {doc_type}"
            if difficulty:
                document_name += f" ({difficulty})"
        
        # æ·»åŠ å…³é”®è¯ä½œä¸ºæ ‡ç­¾
        keywords = metadata.get('keywords', [])
        if keywords:
            document_name += f" - {', '.join(keywords[:3])}"
    
    data = {
        'name': document_name,
        'text': content,
        'indexing_technique': 'high_quality',
        'process_rule': {'mode': 'automatic'}
    }
    
    # å¦‚æœæœ‰å…ƒæ•°æ®ï¼Œæ·»åŠ åˆ°è¯·æ±‚ä¸­
    if metadata:
        data['metadata'] = {
            'url': url,
            'document_type': metadata.get('document_type'),
            'difficulty_level': metadata.get('difficulty_level'),
            'keywords': metadata.get('keywords', []),
            'content_length': metadata.get('content_length'),
            'product_id': metadata.get('product_id')
        }
    
    try:
        # å‘é€ POST è¯·æ±‚
        response = requests.post(api_url, headers=headers, json=data, timeout=config.request_timeout)

        # å¤„ç†å“åº”
        if response.status_code == 201 or response.status_code == 200:
            print(f"[ä»»åŠ¡ 4] æˆåŠŸï¼š{url} å·²åŒæ­¥åˆ° Difyã€‚")
            return True
        else:
            # Kiroï¼šè¯·åœ¨æ­¤å¤„æ·»åŠ æ›´è¯¦ç»†çš„é”™è¯¯å¤„ç†
            print(f"[ä»»åŠ¡ 4] å¤±è´¥ï¼š{url} åŒæ­¥å¤±è´¥ã€‚çŠ¶æ€ç : {response.status_code}, å“åº”: {response.text}")
            return False
            
    except requests.RequestException as e:
        print(f"[ä»»åŠ¡ 4] å¤±è´¥ï¼š{url} åŒæ­¥æ—¶å‘ç”Ÿç½‘ç»œé”™è¯¯: {e}")
        return False



def main():
    """
    ä¸»æ‰§è¡Œå‡½æ•°ï¼šç¼–æ’æ‰€æœ‰ä»»åŠ¡
    """
    # åˆå§‹åŒ–æ—¥å¿—ç³»ç»Ÿ
    logger = setup_logger(
        name="TKE_SYNC",
        log_level=LogLevel.INFO,
        console_output=True,
        file_output=True
    )
    
    # åˆå§‹åŒ–ä¸´æ—¶æ–‡ä»¶ç®¡ç†å™¨
    temp_manager = setup_temp_manager(
        base_prefix="tke_sync_",
        max_age_hours=24,
        max_total_size_mb=512
    )
    
    logger.log_task_start("TKE æ–‡æ¡£åŒæ­¥", version="2.0")
    
    try:
        # åˆå§‹åŒ–é…ç½®ç®¡ç†å™¨
        logger.info("åˆå§‹åŒ–é…ç½®ç®¡ç†å™¨...")
        config_manager = ConfigManager()
        
        # éªŒè¯é…ç½®
        if not config_manager.validate_config():
            logger.error("é…ç½®éªŒè¯å¤±è´¥ï¼Œç¨‹åºç»ˆæ­¢")
            return
        
        config = config_manager.get_config()
        logger.info(f"é…ç½®åŠ è½½æˆåŠŸï¼ŒçŸ¥è¯†åº“æ•°é‡: {len(config.dify_knowledge_base_ids)}")
        
        # åˆå§‹åŒ–çŠ¶æ€ç®¡ç†å™¨
        logger.info(f"ä» {config.state_file} åŠ è½½çŠ¶æ€...")
        state_manager = StateManager(config.state_file)
        crawl_state = state_manager.load_state()
        new_state = crawl_state.copy()
        
        # åˆå§‹åŒ–å†…å®¹æŠ“å–å™¨
        logger.info("åˆå§‹åŒ–å†…å®¹æŠ“å–å™¨...")
        content_scraper = ContentScraper(config)
        
        # åˆå§‹åŒ–å…ƒæ•°æ®ç”Ÿæˆå™¨
        logger.info("åˆå§‹åŒ–å…ƒæ•°æ®ç”Ÿæˆå™¨...")
        metadata_generator = EnhancedMetadataGenerator()
        
        # åˆå§‹åŒ– Dify åŒæ­¥ç®¡ç†å™¨
        logger.info("åˆå§‹åŒ– Dify åŒæ­¥ç®¡ç†å™¨...")
        dify_sync_manager = DifySyncManager(config)
        
        # è®¾ç½®çŸ¥è¯†åº“ç­–ç•¥
        if config.kb_strategy == "all":
            dify_sync_manager.set_strategy(KnowledgeBaseStrategy.ALL)
        elif config.kb_strategy == "round_robin":
            dify_sync_manager.set_strategy(KnowledgeBaseStrategy.ROUND_ROBIN)
        else:
            dify_sync_manager.set_strategy(KnowledgeBaseStrategy.PRIMARY)
        
        # æ„å»º TF-IDF è¯­æ–™åº“
        logger.info("å¼€å§‹æ„å»º TF-IDF è¯­æ–™åº“...")
        corpus_built = False
    
        # è·å–æ‰€æœ‰ URL
        logger.log_task_start("URL å‘ç°")
        all_urls = get_all_doc_urls(config.start_url, config.base_url)
        if not all_urls:
            logger.error("æœªèƒ½è·å–åˆ°ä»»ä½• URLï¼Œä»»åŠ¡ç»ˆæ­¢")
            return
        logger.log_task_complete("URL å‘ç°", 0, url_count=len(all_urls))

        to_update_queue = []  # å¾…æ›´æ–°åˆ—è¡¨

        # éå†ã€æŠ“å–ã€å¯¹æ¯”å“ˆå¸Œå€¼
        logger.log_task_start("å†…å®¹æŠ“å–å’Œå˜æ›´æ£€æµ‹", total_urls=len(all_urls))
        
        for i, url in enumerate(all_urls):
            logger.info(f"å¤„ç†è¿›åº¦ {i+1}/{len(all_urls)}: {url}")
            
            # æŠ“å–å†…å®¹
            content = content_scraper.scrape_content(url)
            if not content:
                logger.warning(f"è·³è¿‡ {url} (æ— æ³•æŠ“å–å†…å®¹)")
                continue
            
            # å°†å†…å®¹æ·»åŠ åˆ°è¯­æ–™åº“ï¼ˆç”¨äº TF-IDF è®¡ç®—ï¼‰
            if not corpus_built:
                metadata_generator.add_document_to_corpus(content)
                
            # è®¡ç®—å“ˆå¸Œå¹¶å¯¹æ¯”
            new_hash = get_content_hash(content)
            old_hash = crawl_state.get(url)

            if new_hash == old_hash:
                logger.debug(f"å†…å®¹æœªå˜æ›´: {url}")
                # å³ä½¿æœªå˜æ›´ï¼Œä¹Ÿå¿…é¡»å°†å…¶ä¿ç•™åœ¨ new_state ä¸­
                new_state[url] = old_hash
            else:
                if old_hash is None:
                    logger.info(f"å‘ç°æ–°æ–‡æ¡£: {url}")
                else:
                    logger.info(f"å†…å®¹å·²å˜æ›´: {url}")
                
                # ç”Ÿæˆå…ƒæ•°æ®
                metadata = metadata_generator.generate_metadata(url, content)
                logger.debug(f"ç”Ÿæˆå…ƒæ•°æ®: ç±»å‹={metadata.get('document_type')}, éš¾åº¦={metadata.get('difficulty_level')}")
                
                # åŠ å…¥å¾…åŒæ­¥é˜Ÿåˆ—
                to_update_queue.append({
                    "url": url,
                    "content": content,
                    "hash": new_hash,
                    "metadata": metadata
                })

        # æ ‡è®°è¯­æ–™åº“æ„å»ºå®Œæˆ
        corpus_built = True
        logger.log_task_complete("å†…å®¹æŠ“å–å’Œå˜æ›´æ£€æµ‹", 0, 
                               processed_urls=len(all_urls), 
                               changed_docs=len(to_update_queue))
        
        # æ‰§è¡ŒåŒæ­¥
        if not to_update_queue:
            logger.info("æ²¡æœ‰æ–‡æ¡£éœ€è¦åŒæ­¥")
        else:
            logger.log_task_start("Dify åŒæ­¥", documents_to_sync=len(to_update_queue))
            
            sync_success_count = 0
            sync_failure_count = 0
            
            for i, item in enumerate(to_update_queue):
                url = item['url']
                content = item['content']
                new_hash = item['hash']
                metadata = item['metadata']
                
                logger.info(f"åŒæ­¥è¿›åº¦ {i+1}/{len(to_update_queue)}: {url}")
                logger.debug(f"å…ƒæ•°æ® - ç±»å‹: {metadata.get('document_type')}, "
                           f"éš¾åº¦: {metadata.get('difficulty_level')}, "
                           f"å…³é”®è¯: {', '.join(metadata.get('keywords', [])[:3])}")
                
                # å…³é”®é€»è¾‘ï¼šåªæœ‰åœ¨ Dify ç¡®è®¤ä¸Šä¼ æˆåŠŸåï¼Œæ‰æ›´æ–°æœ¬åœ°çŠ¶æ€
                try:
                    success = dify_sync_manager.sync_document(url, content, metadata)
                    
                    if success:
                        # ç¡®è®¤æˆåŠŸï¼Œæ›´æ–° new_state ä¸­çš„å“ˆå¸Œå€¼
                        new_state[url] = new_hash
                        sync_success_count += 1
                        logger.info(f"åŒæ­¥æˆåŠŸ: {url}")
                    else:
                        sync_failure_count += 1
                        logger.error(f"åŒæ­¥å¤±è´¥: {url} - æœ¬åœ°çŠ¶æ€å°†ä¸è¢«æ›´æ–°ï¼Œä¸‹æ¬¡è¿è¡Œå°†é‡è¯•")
                        
                except Exception as e:
                    sync_failure_count += 1
                    logger.log_exception(f"åŒæ­¥å¼‚å¸¸: {url}", e)
            
            logger.log_task_complete("Dify åŒæ­¥", 0, 
                                   success_count=sync_success_count,
                                   failure_count=sync_failure_count)

        # ä¿å­˜æœ€ç»ˆçŠ¶æ€
        logger.log_task_start("çŠ¶æ€ä¿å­˜")
        success = state_manager.save_state(new_state)
        if success:
            logger.info(f"çŠ¶æ€å·²æ›´æ–°åˆ° {config.state_file}")
        else:
            logger.error(f"çŠ¶æ€ä¿å­˜å¤±è´¥ï¼Œè¯·æ£€æŸ¥ {config.state_file}")
        
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        state_manager.cleanup_temp_files()
        logger.log_task_complete("çŠ¶æ€ä¿å­˜", 0)
        
        # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
        logger.info("è¾“å‡ºç»Ÿè®¡ä¿¡æ¯...")
        content_scraper.print_stats()
        dify_sync_manager.print_stats()
        state_manager.print_stats()
        temp_manager.print_stats()
        
        # æ‰“å°æ‰§è¡Œæ‘˜è¦
        logger.print_execution_summary()
        
    except Exception as e:
        logger.log_exception("ä¸»ç¨‹åºæ‰§è¡Œå¼‚å¸¸", e)
        raise
    finally:
        # æ¸…ç†èµ„æº
        logger.info("æ¸…ç†èµ„æº...")
        temp_manager.cleanup_all()
        logger.close()


if __name__ == "__main__":
    try:
        main()
    except ConfigurationError as e:
        print(f"\n{e}")
        print(f"\nğŸ”§ é…ç½®å¸®åŠ©:")
        print(f"   è¿è¡Œ 'python test_config.py' è·å–è¯¦ç»†çš„é…ç½®æŒ‡å¯¼")
        exit(1)
    except KeyboardInterrupt:
        print(f"\n\nâš ï¸ ç”¨æˆ·ä¸­æ–­äº†ç¨‹åºæ‰§è¡Œ")
        print(f"ğŸ’¡ ç¨‹åºå·²å®‰å…¨é€€å‡º")
        exit(0)
    except Exception as e:
        print(f"\nâŒ ç¨‹åºæ‰§è¡Œå¼‚å¸¸: {e}")
        print(f"\nğŸ” è°ƒè¯•ä¿¡æ¯:")
        print(f"   1. æ£€æŸ¥é…ç½®æ–‡ä»¶æ˜¯å¦æ­£ç¡®")
        print(f"   2. ç¡®è®¤ç½‘ç»œè¿æ¥æ­£å¸¸")
        print(f"   3. æŸ¥çœ‹æ—¥å¿—æ–‡ä»¶è·å–è¯¦ç»†ä¿¡æ¯")
        print(f"   4. è¿è¡Œ 'python test_config.py' éªŒè¯é…ç½®")
        import traceback
        traceback.print_exc()
        exit(1)